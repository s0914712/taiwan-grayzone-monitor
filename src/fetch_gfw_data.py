#!/usr/bin/env python3
"""
================================================================================
GFW è³‡æ–™æ“·å–è…³æœ¬ - å°ç£å‘¨é‚Šå¯ç–‘èˆ¹éš»ç›£æ¸¬
Taiwan Gray Zone Vessel Monitor - GFW Data Fetcher
================================================================================

åŠŸèƒ½ï¼š
1. å¾ GFW API æ“·å–å°ç£å‘¨é‚Š SAR è¡›æ˜Ÿåµæ¸¬è³‡æ–™ï¼ˆæš—èˆ¹ï¼‰
2. æ“·å–ä¸­åœ‹ç±èˆ¹éš»åœ¨å°ç£å‘¨é‚Šçš„å­˜åœ¨è³‡æ–™ï¼ˆVessel Presenceï¼‰
3. æ“·å–æ¼æ’ˆåŠªåŠ›é‡è³‡æ–™ï¼ˆFishing Effortï¼‰
4. å¤šå€åŸŸæš—èˆ¹åµæ¸¬èˆ‡åˆ†æ
5. è¨ˆç®—å¯ç–‘èˆ¹éš»æŒ‡æ¨™ä¸¦å„²å­˜è‡³ JSON

è³‡æ–™ä¾†æºï¼š
- Global Fishing Watch API (4wings report)
  - public-global-sar-presence:latest
  - public-global-presence:latest (flag=CHN)
  - public-global-fishing-effort:latest
================================================================================
"""

import os
import json
import time
import requests
from datetime import datetime, timedelta
from pathlib import Path

# =============================================================================
# è¨­å®š
# =============================================================================

API_TOKEN = os.environ.get('GFW_API_TOKEN', '').strip()
BASE_URL = "https://gateway.api.globalfishingwatch.org/v3"

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# å°ç£å‘¨é‚Šç›£æ¸¬å€åŸŸï¼ˆç¸½å€åŸŸï¼Œå«æ±æµ·å»¶ä¼¸è‡³ 34Â°Nï¼‰
TAIWAN_AREA = {
    "type": "Polygon",
    "coordinates": [[
        [117.0, 21.0], [130.5, 21.0], [130.5, 34.0], [117.0, 34.0], [117.0, 21.0]
    ]]
}

# æš—èˆ¹åµæ¸¬å­å€åŸŸ
DARK_VESSEL_REGIONS = {
    "taiwan_strait": {
        "name": "å°ç£æµ·å³½",
        "geojson": {
            "type": "Polygon",
            "coordinates": [[
                [118.0, 23.5], [122.0, 23.5], [122.0, 26.5], [118.0, 26.5], [118.0, 23.5]
            ]]
        }
    },
    "east_taiwan": {
        "name": "å°ç£æ±éƒ¨æµ·åŸŸ",
        "geojson": {
            "type": "Polygon",
            "coordinates": [[
                [121.5, 22.0], [124.0, 22.0], [124.0, 25.5], [121.5, 25.5], [121.5, 22.0]
            ]]
        }
    },
    "south_china_sea": {
        "name": "å—æµ·åŒ—éƒ¨",
        "geojson": {
            "type": "Polygon",
            "coordinates": [[
                [110.0, 18.0], [118.0, 18.0], [118.0, 23.0], [110.0, 23.0], [110.0, 18.0]
            ]]
        }
    },
    "east_china_sea": {
        "name": "æ±æµ·",
        "geojson": {
            "type": "Polygon",
            "coordinates": [[
                [122.0, 26.0], [130.5, 26.0], [130.5, 34.0], [122.0, 34.0], [122.0, 26.0]
            ]]
        }
    }
}

# è»æ¼”å€åŸŸå®šç¾©ï¼ˆJoint Sword ç­‰ï¼‰
DRILL_ZONES = {
    "north": {"name": "åŒ—å€", "coords": [[121.0, 25.5], [122.5, 25.5], [122.5, 26.8], [121.0, 26.8], [121.0, 25.5]]},
    "east": {"name": "æ±å€", "coords": [[122.5, 23.0], [125.0, 23.0], [125.0, 25.5], [122.5, 25.5], [122.5, 23.0]]},
    "south": {"name": "å—å€", "coords": [[119.0, 21.5], [121.0, 21.5], [121.0, 23.0], [119.0, 23.0], [119.0, 21.5]]},
    "west": {"name": "è¥¿å€", "coords": [[118.5, 23.5], [120.0, 23.5], [120.0, 25.0], [118.5, 25.0], [118.5, 23.5]]},
}

# =============================================================================
# API å‡½æ•¸
# =============================================================================

def get_headers():
    """Build request headers"""
    return {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }


def fetch_4wings_report(dataset, region, start_date, end_date, filters=None,
                        spatial_resolution="HIGH", spatial_aggregation="false",
                        group_by=None):
    """
    é€šç”¨ 4wings report API å‘¼å«
    """
    params = {
        "datasets[0]": dataset,
        "date-range": f"{start_date},{end_date}",
        "temporal-resolution": "DAILY",
        "spatial-resolution": spatial_resolution,
        "spatial-aggregation": spatial_aggregation,
        "format": "JSON"
    }

    if filters:
        for i, f in enumerate(filters):
            params[f"filters[{i}]"] = f

    if group_by:
        params["group-by"] = group_by

    try:
        response = requests.post(
            f"{BASE_URL}/4wings/report",
            params=params,
            json={"geojson": region},
            headers=get_headers(),
            timeout=120
        )

        if response.status_code == 200:
            return response.json()
        else:
            print(f"   âŒ API éŒ¯èª¤ {response.status_code}: {response.text[:300]}")
            return {}

    except Exception as e:
        print(f"   âŒ è«‹æ±‚å¤±æ•—: {e}")
        return {}


def parse_4wings_entries(data):
    """è§£æ 4wings API å›æ‡‰çš„ entries"""
    entries = data.get('entries', [])
    if not entries:
        return []

    results = []
    for entry in entries:
        for key, values in entry.items():
            if isinstance(values, list):
                results.extend(values)
    return results


# =============================================================================
# è³‡æ–™æ“·å–å‡½æ•¸
# =============================================================================

def fetch_sar_data(region, start_date, end_date):
    """æ“·å– SAR è¡›æ˜Ÿåµæ¸¬è³‡æ–™ï¼ˆå…¨éƒ¨ï¼‰"""
    print("   ğŸ›°ï¸ SAR è¡›æ˜Ÿåµæ¸¬...")
    data = fetch_4wings_report(
        "public-global-sar-presence:latest",
        region, start_date, end_date
    )
    records = parse_4wings_entries(data)
    print(f"      å–å¾— {len(records)} ç­† SAR è¨˜éŒ„")
    return records


def fetch_vessel_presence(region, start_date, end_date):
    """æ“·å–ä¸­åœ‹ç±èˆ¹éš»å­˜åœ¨è³‡æ–™ï¼ˆCHN flag filterï¼‰"""
    print("   ğŸš¢ ä¸­åœ‹ç±èˆ¹éš»å­˜åœ¨...")
    data = fetch_4wings_report(
        "public-global-presence:latest",
        region, start_date, end_date,
        filters=["flag='CHN'"]
    )
    records = parse_4wings_entries(data)
    print(f"      å–å¾— {len(records)} ç­†ä¸­åœ‹èˆ¹éš»è¨˜éŒ„")
    return records


def fetch_fishing_effort(region, start_date, end_date):
    """æ“·å–æ¼æ’ˆåŠªåŠ›é‡è³‡æ–™"""
    print("   ğŸ£ æ¼æ’ˆåŠªåŠ›é‡...")
    data = fetch_4wings_report(
        "public-global-fishing-effort:latest",
        region, start_date, end_date
    )
    records = parse_4wings_entries(data)
    print(f"      å–å¾— {len(records)} ç­†æ¼æ’ˆè¨˜éŒ„")
    return records


def fetch_fishing_effort_by_flag(region, start_date, end_date):
    """æ“·å–æ¼æ’ˆåŠªåŠ›é‡ï¼ˆæŒ‰åœ‹æ——åˆ†çµ„ï¼‰"""
    print("   ğŸ£ æ¼æ’ˆåŠªåŠ›é‡ï¼ˆæŒ‰åœ‹æ——ï¼‰...")
    data = fetch_4wings_report(
        "public-global-fishing-effort:latest",
        region, start_date, end_date,
        spatial_resolution="LOW",
        spatial_aggregation="true",
        group_by="FLAG"
    )
    records = parse_4wings_entries(data)
    print(f"      å–å¾— {len(records)} ç­†è¨˜éŒ„")
    return records


# =============================================================================
# åˆ†æå‡½æ•¸
# =============================================================================

def is_in_drill_zone(lat, lon):
    """æª¢æŸ¥åº§æ¨™æ˜¯å¦åœ¨ä»»ä½•è»æ¼”å€å…§"""
    for zone_id, zone in DRILL_ZONES.items():
        coords = zone['coords']
        lons = [c[0] for c in coords]
        lats = [c[1] for c in coords]
        if min(lats) <= lat <= max(lats) and min(lons) <= lon <= max(lons):
            return zone_id
    return None


def analyze_sar_daily(sar_records):
    """å°‡ SAR è¨˜éŒ„å½™æ•´ç‚ºæ¯æ—¥çµ±è¨ˆ"""
    daily_stats = {}
    for record in sar_records:
        date = record.get('date', '')[:10]
        if not date:
            continue

        if date not in daily_stats:
            daily_stats[date] = {
                'date': date,
                'total_detections': 0,
                'dark_vessels': 0,
            }

        daily_stats[date]['total_detections'] += 1

        if not record.get('vesselId'):
            daily_stats[date]['dark_vessels'] += 1

    return sorted(daily_stats.values(), key=lambda x: x['date'])


def analyze_presence(presence_records):
    """åˆ†æä¸­åœ‹èˆ¹éš»åœ¨å°ç£å‘¨é‚Šçš„å­˜åœ¨æƒ…æ³"""
    daily_presence = {}
    drill_zone_records = 0
    total_hours = 0

    for record in presence_records:
        date = record.get('date', '')[:10]
        if not date:
            continue

        hours = record.get('hours', record.get('value', 0))
        if not isinstance(hours, (int, float)):
            hours = 0

        if date not in daily_presence:
            daily_presence[date] = {
                'date': date,
                'chn_vessel_hours': 0,
                'in_drill_zone_hours': 0,
            }

        daily_presence[date]['chn_vessel_hours'] += hours
        total_hours += hours

        lat = record.get('lat', record.get('latitude'))
        lon = record.get('lon', record.get('longitude'))
        if lat is not None and lon is not None:
            zone = is_in_drill_zone(lat, lon)
            if zone:
                daily_presence[date]['in_drill_zone_hours'] += hours
                drill_zone_records += 1

    return {
        'daily': sorted(daily_presence.values(), key=lambda x: x['date']),
        'total_records': len(presence_records),
        'total_hours': round(total_hours, 1),
        'drill_zone_records': drill_zone_records,
    }


def analyze_fishing(fishing_records):
    """åˆ†ææ¼æ’ˆåŠªåŠ›é‡"""
    daily_effort = {}
    total_hours = 0

    for record in fishing_records:
        date = record.get('date', '')[:10]
        if not date:
            continue

        hours = record.get('hours', record.get('value', 0))
        if not isinstance(hours, (int, float)):
            hours = 0

        if date not in daily_effort:
            daily_effort[date] = {
                'date': date,
                'fishing_hours': 0,
            }

        daily_effort[date]['fishing_hours'] += hours
        total_hours += hours

    return {
        'daily': sorted(daily_effort.values(), key=lambda x: x['date']),
        'total_fishing_hours': round(total_hours, 1),
    }


# =============================================================================
# æš—èˆ¹åµæ¸¬ï¼ˆå¤šå€åŸŸï¼‰
# =============================================================================

def detect_dark_vessels_in_region(region_geojson, start_date, end_date):
    """
    åµæ¸¬æŒ‡å®šå€åŸŸçš„æš—èˆ¹
    æš—èˆ¹å®šç¾©ï¼šSAR åµæ¸¬åˆ°ä½†ç„¡ AIS åŒ¹é…ï¼ˆvesselId ç‚ºç©ºï¼‰
    """
    records = fetch_sar_data(region_geojson, start_date, end_date)

    dark_vessels = []
    matched_vessels = []

    for d in records:
        vessel_id = d.get('vesselId', '')
        if not vessel_id:
            dark_vessels.append(d)
        else:
            matched_vessels.append(d)

    # æš—èˆ¹æŒ‰æ—¥æœŸåˆ†çµ„
    dark_by_date = {}
    for d in dark_vessels:
        date = d.get('date', '')[:10]
        if date:
            dark_by_date[date] = dark_by_date.get(date, 0) + d.get('detections', 1)

    # æœ‰ AIS çš„èˆ¹éš»æŒ‰åœ‹æ——åˆ†çµ„
    matched_by_flag = {}
    for d in matched_vessels:
        flag = d.get('flag', 'Unknown') or 'Unknown'
        matched_by_flag[flag] = matched_by_flag.get(flag, 0) + d.get('detections', 1)

    # æš—èˆ¹ä½ç½®è©³æƒ…ï¼ˆé™åˆ¶å‰ 100 ç­†é¿å…è³‡æ–™éå¤§ï¼‰
    dark_details = []
    for d in dark_vessels[:100]:
        lat = d.get('lat', d.get('latitude'))
        lon = d.get('lon', d.get('longitude'))
        if lat is not None and lon is not None:
            dark_details.append({
                'lat': lat,
                'lon': lon,
                'date': d.get('date', '')[:10],
                'detections': d.get('detections', 1),
            })

    total = len(records)
    return {
        'total_detections': total,
        'dark_vessels': len(dark_vessels),
        'matched_vessels': len(matched_vessels),
        'dark_ratio': round(len(dark_vessels) / total * 100, 1) if total > 0 else 0,
        'dark_by_date': dict(sorted(dark_by_date.items())),
        'matched_by_flag': dict(sorted(matched_by_flag.items(), key=lambda x: x[1], reverse=True)),
        'dark_details': dark_details,
    }


def run_dark_vessel_analysis(start_date, end_date):
    """
    å°æ‰€æœ‰ç›£æ¸¬å€åŸŸåŸ·è¡Œæš—èˆ¹åµæ¸¬åˆ†æ
    """
    print("\nğŸ”¦ æš—èˆ¹åµæ¸¬åˆ†æï¼ˆå¤šå€åŸŸï¼‰...")

    regions_result = {}
    overall_dark = 0
    overall_total = 0
    overall_dark_by_date = {}

    for region_id, region_info in DARK_VESSEL_REGIONS.items():
        print(f"\n   ğŸ“ {region_info['name']}...")
        result = detect_dark_vessels_in_region(
            region_info['geojson'], start_date, end_date
        )
        result['name'] = region_info['name']
        regions_result[region_id] = result

        overall_dark += result['dark_vessels']
        overall_total += result['total_detections']

        # åˆä½µæ—¥æœŸçµ±è¨ˆ
        for date, count in result['dark_by_date'].items():
            overall_dark_by_date[date] = overall_dark_by_date.get(date, 0) + count

        print(f"      ç¸½åµæ¸¬: {result['total_detections']}, "
              f"æš—èˆ¹: {result['dark_vessels']}, "
              f"æ¯”ä¾‹: {result['dark_ratio']}%")

        # é¿å… API é€Ÿç‡é™åˆ¶
        time.sleep(2)

    output = {
        'updated_at': datetime.utcnow().isoformat() + 'Z',
        'data_range': {'start': start_date, 'end': end_date},
        'overall': {
            'total_detections': overall_total,
            'dark_vessels': overall_dark,
            'dark_ratio': round(overall_dark / overall_total * 100, 1) if overall_total > 0 else 0,
            'dark_by_date': dict(sorted(overall_dark_by_date.items())),
        },
        'regions': regions_result,
    }

    # å„²å­˜æš—èˆ¹è³‡æ–™
    output_path = DATA_DIR / 'dark_vessels.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\n   âœ… æš—èˆ¹è³‡æ–™å·²å„²å­˜: {output_path}")
    print(f"      ç¸½åµæ¸¬: {overall_total}, æš—èˆ¹: {overall_dark}, "
          f"æ¯”ä¾‹: {output['overall']['dark_ratio']}%")

    return output


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================

def main():
    print("=" * 60)
    print("ğŸ›°ï¸ GFW è³‡æ–™æ“·å– - å°ç£å‘¨é‚Šå¯ç–‘èˆ¹éš»ç›£æ¸¬")
    print("=" * 60)
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")

    if not API_TOKEN:
        print("âš ï¸ æœªè¨­å®š GFW_API_TOKENï¼Œè·³é GFW è³‡æ–™æ”¶é›†")
        return

    # è¨ˆç®—æ—¥æœŸç¯„åœï¼ˆæœ€è¿‘ 30 å¤©ï¼‰
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=30)

    start_str = start_date.strftime('%Y-%m-%d')
    end_str = end_date.strftime('%Y-%m-%d')

    print(f"\nğŸ“… æŸ¥è©¢ç¯„åœ: {start_str} ~ {end_str}")

    # â”€â”€ ç¬¬ä¸€éƒ¨åˆ†ï¼šç¸½å€åŸŸä¸‰çµ„è³‡æ–™é›† â”€â”€
    print(f"\nğŸ“¡ æ“·å– GFW è³‡æ–™ï¼ˆä¸‰çµ„è³‡æ–™é›†ï¼‰...")

    sar_records = fetch_sar_data(TAIWAN_AREA, start_str, end_str)
    presence_records = fetch_vessel_presence(TAIWAN_AREA, start_str, end_str)
    fishing_records = fetch_fishing_effort(TAIWAN_AREA, start_str, end_str)

    daily_list = analyze_sar_daily(sar_records)
    presence_analysis = analyze_presence(presence_records)
    fishing_analysis = analyze_fishing(fishing_records)

    # è¨ˆç®—æš—èˆ¹è¶¨å‹¢
    if len(daily_list) >= 7:
        recent_7d = sum(d['dark_vessels'] for d in daily_list[-7:]) / 7
        previous_7d = sum(d['dark_vessels'] for d in daily_list[-14:-7]) / 7 if len(daily_list) >= 14 else recent_7d
        trend = ((recent_7d - previous_7d) / previous_7d * 100) if previous_7d > 0 else 0
    else:
        recent_7d = 0
        trend = 0

    output = {
        'updated_at': datetime.utcnow().isoformat() + 'Z',
        'data_range': {'start': start_str, 'end': end_str},
        'summary': {
            'total_days': len(daily_list),
            'avg_daily_detections': sum(d['total_detections'] for d in daily_list) / len(daily_list) if daily_list else 0,
            'avg_daily_dark_vessels': sum(d['dark_vessels'] for d in daily_list) / len(daily_list) if daily_list else 0,
            'recent_7d_avg': recent_7d,
            'trend_pct': trend,
            'chn_presence_records': presence_analysis['total_records'],
            'chn_presence_hours': presence_analysis['total_hours'],
            'chn_drill_zone_records': presence_analysis['drill_zone_records'],
            'total_fishing_hours': fishing_analysis['total_fishing_hours'],
        },
        'daily': daily_list,
        'chn_presence': presence_analysis,
        'fishing_effort': fishing_analysis,
        'drill_zones': DRILL_ZONES,
        'alerts': []
    }

    # æª¢æŸ¥æš—èˆ¹ç•°å¸¸
    if len(daily_list) >= 2:
        latest = daily_list[-1]
        avg = output['summary']['avg_daily_dark_vessels']
        if avg > 0 and latest['dark_vessels'] > avg * 1.5:
            output['alerts'].append({
                'type': 'high_dark_vessels',
                'date': latest['date'],
                'value': latest['dark_vessels'],
                'threshold': avg * 1.5,
                'message': f"æš—èˆ¹æ•¸é‡ç•°å¸¸å¢åŠ : {latest['dark_vessels']} (å¹³å‡ {avg:.0f})"
            })

    if presence_analysis['drill_zone_records'] > 0:
        output['alerts'].append({
            'type': 'chn_drill_zone_presence',
            'value': presence_analysis['drill_zone_records'],
            'message': f"ä¸­åœ‹èˆ¹éš»åœ¨è»æ¼”å€æ´»å‹•: {presence_analysis['drill_zone_records']} ç­†è¨˜éŒ„"
        })

    output_path = DATA_DIR / 'vessel_data.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… è³‡æ–™å·²å„²å­˜: {output_path}")
    print(f"   SAR åµæ¸¬: {len(sar_records)} ç­†")
    print(f"   ä¸­åœ‹èˆ¹éš»: {presence_analysis['total_records']} ç­† "
          f"(è»æ¼”å€ {presence_analysis['drill_zone_records']} ç­†)")
    print(f"   æ¼æ’ˆåŠªåŠ›: {fishing_analysis['total_fishing_hours']:.0f} å°æ™‚")
    print(f"   æš—èˆ¹è¶¨å‹¢: {trend:+.1f}%")

    # â”€â”€ ç¬¬äºŒéƒ¨åˆ†ï¼šå¤šå€åŸŸæš—èˆ¹åµæ¸¬ â”€â”€
    run_dark_vessel_analysis(start_str, end_str)


if __name__ == "__main__":
    main()
