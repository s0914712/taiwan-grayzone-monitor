#!/usr/bin/env python3
"""
================================================================================
GFW è³‡æ–™æ“·å–è…³æœ¬ - å°ç£å‘¨é‚Šå¯ç–‘èˆ¹éš»ç›£æ¸¬
Taiwan Gray Zone Vessel Monitor - GFW Data Fetcher
================================================================================

åŠŸèƒ½ï¼š
1. å¾ GFW API æ“·å–å°ç£å‘¨é‚Š SAR è¡›æ˜Ÿåµæ¸¬è³‡æ–™
2. è¨ˆç®—å¯ç–‘èˆ¹éš»æŒ‡æ¨™ï¼ˆæš—èˆ¹ã€è»æ¼”å€æ´»å‹•ï¼‰
3. å„²å­˜è‡³ JSON ä¾›å‰ç«¯ä½¿ç”¨

è³‡æ–™ä¾†æºï¼š
- Global Fishing Watch API (SAR Presence)
- ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Š (å¾…æ•´åˆ)
================================================================================
"""

import os
import json
import requests
from datetime import datetime, timedelta
from pathlib import Path

# =============================================================================
# è¨­å®š
# =============================================================================

API_TOKEN = os.environ.get('GFW_API_TOKEN', '')
BASE_URL = "https://gateway.api.globalfishingwatch.org/v3"

HEADERS = {
    "Authorization": f"Bearer {API_TOKEN}",
    "Content-Type": "application/json"
}

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# å°ç£å‘¨é‚Šç›£æ¸¬å€åŸŸ
TAIWAN_AREA = {
    "type": "Polygon",
    "coordinates": [[
        [117.0, 21.0], [126.0, 21.0], [126.0, 27.0], [117.0, 27.0], [117.0, 21.0]
    ]]
}

# è»æ¼”å€åŸŸå®šç¾©ï¼ˆJoint Sword ç­‰ï¼‰
DRILL_ZONES = {
    "north": {"name": "åŒ—å€", "coords": [[121.0, 25.5], [122.5, 25.5], [122.5, 26.8], [121.0, 26.8], [121.0, 25.5]]},
    "east": {"name": "æ±å€", "coords": [[122.5, 23.0], [125.0, 23.0], [125.0, 25.5], [122.5, 25.5], [122.5, 23.0]]},
    "south": {"name": "å—å€", "coords": [[119.0, 21.5], [121.0, 21.5], [121.0, 23.0], [119.0, 23.0], [119.0, 21.5]]},
    "west": {"name": "è¥¿å€", "coords": [[118.5, 23.5], [120.0, 23.5], [120.0, 25.0], [118.5, 25.0], [118.5, 23.5]]},
}

# =============================================================================
# API å‡½æ•¸
# =============================================================================

def fetch_sar_data(region: dict, start_date: str, end_date: str) -> dict:
    """æ“·å– SAR è¡›æ˜Ÿåµæ¸¬è³‡æ–™"""
    
    params = {
        "datasets[0]": "public-global-sar-presence:latest",
        "date-range": f"{start_date},{end_date}",
        "temporal-resolution": "DAILY",
        "spatial-resolution": "HIGH",
        "spatial-aggregation": "false",
        "format": "JSON"
    }
    
    try:
        response = requests.post(
            f"{BASE_URL}/4wings/report",
            params=params,
            json={"geojson": region},
            headers=HEADERS,
            timeout=120
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âŒ API éŒ¯èª¤ {response.status_code}: {response.text[:200]}")
            return {}
            
    except Exception as e:
        print(f"âŒ è«‹æ±‚å¤±æ•—: {e}")
        return {}


def parse_sar_response(data: dict) -> list:
    """è§£æ SAR API å›æ‡‰"""
    
    entries = data.get('entries', [])
    if not entries:
        return []
    
    results = []
    for entry in entries:
        for key, values in entry.items():
            if isinstance(values, list):
                for item in values:
                    results.append(item)
    
    return results


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================

def main():
    print("="*60)
    print("ğŸ›°ï¸ GFW è³‡æ–™æ“·å– - å°ç£å‘¨é‚Šå¯ç–‘èˆ¹éš»ç›£æ¸¬")
    print("="*60)
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")
    
    if not API_TOKEN:
        print("âš ï¸ æœªè¨­å®š GFW_API_TOKENï¼Œè·³é GFW è³‡æ–™æ”¶é›†")
        return
    
    # è¨ˆç®—æ—¥æœŸç¯„åœï¼ˆæœ€è¿‘ 30 å¤©ï¼‰
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=30)
    
    start_str = start_date.strftime('%Y-%m-%d')
    end_str = end_date.strftime('%Y-%m-%d')
    
    print(f"\nğŸ“… æŸ¥è©¢ç¯„åœ: {start_str} ~ {end_str}")
    
    # æ“·å– SAR è³‡æ–™
    print("\nğŸ“¡ æ“·å– SAR è¡›æ˜Ÿåµæ¸¬è³‡æ–™...")
    sar_data = fetch_sar_data(TAIWAN_AREA, start_str, end_str)
    sar_records = parse_sar_response(sar_data)
    
    print(f"   å–å¾— {len(sar_records)} ç­†åµæ¸¬è¨˜éŒ„")
    
    # æŒ‰æ—¥æœŸå½™ç¸½
    daily_stats = {}
    for record in sar_records:
        date = record.get('date', '')[:10]
        if not date:
            continue
        
        if date not in daily_stats:
            daily_stats[date] = {
                'date': date,
                'total_detections': 0,
                'dark_vessels': 0,
            }
        
        daily_stats[date]['total_detections'] += 1
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºæš—èˆ¹ï¼ˆç„¡ vessel IDï¼‰
        if not record.get('vesselId'):
            daily_stats[date]['dark_vessels'] += 1
    
    # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦æ’åº
    daily_list = sorted(daily_stats.values(), key=lambda x: x['date'])
    
    # è¨ˆç®—è¶¨å‹¢
    if len(daily_list) >= 7:
        recent_7d = sum(d['dark_vessels'] for d in daily_list[-7:]) / 7
        previous_7d = sum(d['dark_vessels'] for d in daily_list[-14:-7]) / 7 if len(daily_list) >= 14 else recent_7d
        trend = ((recent_7d - previous_7d) / previous_7d * 100) if previous_7d > 0 else 0
    else:
        recent_7d = 0
        trend = 0
    
    # å»ºç«‹è¼¸å‡ºè³‡æ–™
    output = {
        'updated_at': datetime.utcnow().isoformat() + 'Z',
        'data_range': {
            'start': start_str,
            'end': end_str
        },
        'summary': {
            'total_days': len(daily_list),
            'avg_daily_detections': sum(d['total_detections'] for d in daily_list) / len(daily_list) if daily_list else 0,
            'avg_daily_dark_vessels': sum(d['dark_vessels'] for d in daily_list) / len(daily_list) if daily_list else 0,
            'recent_7d_avg': recent_7d,
            'trend_pct': trend
        },
        'daily': daily_list,
        'drill_zones': DRILL_ZONES,
        'alerts': []
    }
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç•°å¸¸
    if len(daily_list) >= 2:
        latest = daily_list[-1]
        avg = output['summary']['avg_daily_dark_vessels']
        if latest['dark_vessels'] > avg * 1.5:
            output['alerts'].append({
                'type': 'high_dark_vessels',
                'date': latest['date'],
                'value': latest['dark_vessels'],
                'threshold': avg * 1.5,
                'message': f"æš—èˆ¹æ•¸é‡ç•°å¸¸å¢åŠ : {latest['dark_vessels']} (å¹³å‡ {avg:.0f})"
            })
    
    # å„²å­˜è³‡æ–™
    output_path = DATA_DIR / 'vessel_data.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… è³‡æ–™å·²å„²å­˜: {output_path}")
    print(f"   å¹³å‡æ¯æ—¥åµæ¸¬: {output['summary']['avg_daily_detections']:.0f}")
    print(f"   å¹³å‡æ¯æ—¥æš—èˆ¹: {output['summary']['avg_daily_dark_vessels']:.0f}")
    print(f"   7æ—¥è¶¨å‹¢: {trend:+.1f}%")


if __name__ == "__main__":
    main()
