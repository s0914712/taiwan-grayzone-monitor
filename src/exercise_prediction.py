#!/usr/bin/env python3
"""
================================================================================
è»æ¼”é æ¸¬åˆ†æï¼šæš—èˆ¹æ´»å‹•ä½œç‚ºè»æ¼”é è­¦æŒ‡æ¨™
Exercise Prediction: Dark Vessel Activity as Early Warning Indicator
================================================================================

æ¯æ—¥è‡ªå‹•åŸ·è¡Œï¼š
  1. ä¸‹è¼‰ PLA æ¶æ¬¡è³‡æ–™ (JapanandBattleship.csv)
  2. è®€å–æœ¬åœ°æš—èˆ¹åµæ¸¬è³‡æ–™ (vessel_data.json / dark_vessels.json)
  3. åˆä½µæ—¥æœŸï¼Œè¨ˆç®—æš—èˆ¹ vs æ¶æ¬¡çš„ç›¸é—œæ€§èˆ‡ Granger å› æœ
  4. é€²è¡Œè»æ¼”äº‹ä»¶ç ”ç©¶
  5. å„²å­˜çµæœè‡³ data/exercise_prediction.json
================================================================================
"""

import json
import io
import requests
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# PLA æ¶æ¬¡ CSV ä¾†æº
PLA_CSV_URL = (
    "https://raw.githubusercontent.com/s0914712/pla-data-dashboard"
    "/main/data/JapanandBattleship.csv"
)

# å·²çŸ¥è»æ¼”äº‹ä»¶
MILITARY_EXERCISES = [
    {
        "name": "Joint Sword 2024A",
        "start": "2024-05-23",
        "end": "2024-05-24",
        "trigger": "è³´æ¸…å¾·å°±è·",
    },
    {
        "name": "Joint Sword 2024B",
        "start": "2024-10-14",
        "end": "2024-10-15",
        "trigger": "é›™åç¯€æ¼”èªª",
    },
    {
        "name": "2024-12 è»æ¼”",
        "start": "2024-12-09",
        "end": "2024-12-11",
        "trigger": "å¹´åº¦ä¾‹è¡Œ",
    },
    {
        "name": "2025-12 è»æ¼”",
        "start": "2025-12-29",
        "end": "2025-12-31",
        "trigger": "å¹´åº¦ä¾‹è¡Œ",
    },
]

MAX_LAG = 14
MAX_LAG_GRANGER = 7


# =============================================================================
# è³‡æ–™è¼‰å…¥
# =============================================================================


def fetch_pla_sorties() -> pd.DataFrame:
    """å¾ GitHub ä¸‹è¼‰ PLA æ¶æ¬¡ CSV"""
    print("   ğŸ“¥ ä¸‹è¼‰ PLA æ¶æ¬¡è³‡æ–™...")
    try:
        resp = requests.get(PLA_CSV_URL, timeout=30)
        resp.raise_for_status()
    except requests.RequestException as e:
        print(f"   âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        return pd.DataFrame()

    df = pd.read_csv(io.StringIO(resp.text))
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date"])
    df = df.rename(
        columns={
            "pla_aircraft_sorties": "sorties",
            "plan_vessel_sorties": "vessels",
        }
    )
    df["sorties"] = pd.to_numeric(df["sorties"], errors="coerce").fillna(0)
    df["vessels"] = pd.to_numeric(df["vessels"], errors="coerce").fillna(0)
    print(f"      {len(df)} ç­† ({df['date'].min():%Y-%m-%d} ~ {df['date'].max():%Y-%m-%d})")
    return df


def load_dark_vessel_daily() -> pd.DataFrame:
    """
    å¾æœ¬åœ° JSON è¼‰å…¥æ¯æ—¥æš—èˆ¹çµ±è¨ˆã€‚
    å„ªå…ˆä½¿ç”¨ dark_vessels.json (overall.dark_by_date)ï¼Œ
    å…¶æ¬¡ä½¿ç”¨ vessel_data.json (daily)ã€‚
    """
    rows = []

    # ä¾†æº 1: dark_vessels.json (è·¨å€åŸŸå½™ç¸½)
    dark_path = DATA_DIR / "dark_vessels.json"
    if dark_path.exists():
        with open(dark_path, "r", encoding="utf-8") as f:
            dv = json.load(f)
        for date_str, count in dv.get("overall", {}).get("dark_by_date", {}).items():
            rows.append({"date": date_str, "dark_vessels": count})

    # ä¾†æº 2: vessel_data.json (SAR daily)
    vessel_path = DATA_DIR / "vessel_data.json"
    if vessel_path.exists():
        with open(vessel_path, "r", encoding="utf-8") as f:
            vd = json.load(f)
        for entry in vd.get("daily", []):
            rows.append({
                "date": entry["date"],
                "dark_vessels": entry.get("dark_vessels", 0),
            })

    if not rows:
        print("   âš ï¸ æ‰¾ä¸åˆ°æš—èˆ¹è³‡æ–™")
        return pd.DataFrame()

    df = pd.DataFrame(rows)
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date"])
    # åŒä¸€å¤©å–æœ€å¤§å€¼ï¼ˆå…©å€‹ä¾†æºå¯èƒ½é‡ç–Šï¼‰
    df = df.groupby("date", as_index=False)["dark_vessels"].max()
    df = df.sort_values("date").reset_index(drop=True)
    print(f"   ğŸ›°ï¸ æš—èˆ¹è³‡æ–™: {len(df)} å¤© ({df['date'].min():%Y-%m-%d} ~ {df['date'].max():%Y-%m-%d})")
    return df


# =============================================================================
# åˆ†æå‡½æ•¸
# =============================================================================


def lag_correlation(dark: pd.Series, sorties: pd.Series, max_lag: int = MAX_LAG):
    """
    è¨ˆç®—æš—èˆ¹ vs æ¶æ¬¡çš„æ™‚é–“æ»¯å¾Œç›¸é—œä¿‚æ•¸ã€‚
    æ­£æ•¸ lag = æš—èˆ¹é ˜å…ˆï¼ˆæš—èˆ¹ t-lag â†’ æ¶æ¬¡ tï¼‰ã€‚
    """
    dark_z = (dark - dark.mean()) / dark.std() if dark.std() > 0 else dark * 0
    sort_z = (sorties - sorties.mean()) / sorties.std() if sorties.std() > 0 else sorties * 0

    results = []
    for lag in range(-max_lag, max_lag + 1):
        if lag < 0:
            r = dark_z.iloc[:lag].reset_index(drop=True).corr(
                sort_z.iloc[-lag:].reset_index(drop=True)
            )
        elif lag > 0:
            r = dark_z.iloc[lag:].reset_index(drop=True).corr(
                sort_z.iloc[:-lag].reset_index(drop=True)
            )
        else:
            r = dark_z.corr(sort_z)
        if not np.isnan(r):
            results.append({"lag": lag, "correlation": round(float(r), 6)})
    return results


def granger_test(merged: pd.DataFrame):
    """
    Granger å› æœæª¢é©—ï¼ˆé›™å‘ï¼‰ã€‚
    éœ€è¦ scipy èˆ‡ statsmodelsã€‚
    """
    try:
        from statsmodels.tsa.stattools import grangercausalitytests, adfuller
    except ImportError:
        print("   âš ï¸ statsmodels æœªå®‰è£ï¼Œè·³é Granger æª¢é©—")
        return {}

    results = {"dark_to_sorties": {}, "sorties_to_dark": {}}

    # å¹³ç©©æ€§æª¢æŸ¥
    def is_stationary(series):
        if len(series.dropna()) < 20:
            return False
        adf = adfuller(series.dropna(), autolag="AIC")
        return adf[1] < 0.05

    dark_stat = is_stationary(merged["dark_vessels"])
    sort_stat = is_stationary(merged["sorties"])

    if not dark_stat or not sort_stat:
        merged = merged.copy()
        merged["dark_vessels"] = merged["dark_vessels"].diff()
        merged["sorties"] = merged["sorties"].diff()

    data = merged[["sorties", "dark_vessels"]].dropna()
    # è‡ªé©æ‡‰æœ€å¤§ lagï¼šè‡³å°‘éœ€è¦ 3*lag ç­†è³‡æ–™
    usable_lag = min(MAX_LAG_GRANGER, max(1, len(data) // 3 - 1))
    if len(data) < usable_lag + 5:
        print("   âš ï¸ è³‡æ–™ä¸è¶³ï¼Œè·³é Granger æª¢é©—")
        return results

    print(f"      ä½¿ç”¨ maxlag={usable_lag} (è³‡æ–™ {len(data)} ç­†)")

    # æš—èˆ¹ â†’ æ¶æ¬¡
    try:
        gc = grangercausalitytests(data.values, maxlag=usable_lag, verbose=False)
        for lag in range(1, usable_lag + 1):
            f_stat = gc[lag][0]["ssr_ftest"][0]
            p_val = gc[lag][0]["ssr_ftest"][1]
            results["dark_to_sorties"][str(lag)] = {
                "f_stat": round(float(f_stat), 4),
                "p_value": round(float(p_val), 6),
                "significant": bool(p_val < 0.05),
            }
    except Exception as e:
        print(f"   âš ï¸ darkâ†’sorties æª¢é©—å¤±æ•—: {e}")

    # æ¶æ¬¡ â†’ æš—èˆ¹
    try:
        data_rev = data[["dark_vessels", "sorties"]]
        gc2 = grangercausalitytests(data_rev.values, maxlag=usable_lag, verbose=False)
        for lag in range(1, usable_lag + 1):
            f_stat = gc2[lag][0]["ssr_ftest"][0]
            p_val = gc2[lag][0]["ssr_ftest"][1]
            results["sorties_to_dark"][str(lag)] = {
                "f_stat": round(float(f_stat), 4),
                "p_value": round(float(p_val), 6),
                "significant": bool(p_val < 0.05),
            }
    except Exception as e:
        print(f"   âš ï¸ sortiesâ†’dark æª¢é©—å¤±æ•—: {e}")

    return results


def event_study(merged: pd.DataFrame):
    """è»æ¼”äº‹ä»¶ç ”ç©¶ï¼šæ¯”è¼ƒè»æ¼”å‰ 14 å¤© vs è»æ¼”æœŸé–“çš„æš—èˆ¹èˆ‡æ¶æ¬¡ã€‚"""
    results = []
    for ex in MILITARY_EXERCISES:
        ex_start = pd.to_datetime(ex["start"])
        ex_end = pd.to_datetime(ex["end"])

        before = merged[
            (merged["date"] >= ex_start - timedelta(days=14))
            & (merged["date"] < ex_start)
        ]
        during = merged[
            (merged["date"] >= ex_start) & (merged["date"] <= ex_end)
        ]
        after = merged[
            (merged["date"] > ex_end)
            & (merged["date"] <= ex_end + timedelta(days=7))
        ]

        def safe_mean(s):
            return round(float(s.mean()), 2) if len(s) > 0 else None

        before_dark = safe_mean(before["dark_vessels"])
        during_dark = safe_mean(during["dark_vessels"])
        change_pct = None
        if before_dark and before_dark > 0 and during_dark is not None:
            change_pct = round((during_dark - before_dark) / before_dark * 100, 1)

        results.append({
            "exercise": ex["name"],
            "trigger": ex["trigger"],
            "start": ex["start"],
            "end": ex["end"],
            "before_dark_avg": before_dark,
            "during_dark_avg": during_dark,
            "after_dark_avg": safe_mean(after["dark_vessels"]),
            "dark_change_pct": change_pct,
            "before_sorties_avg": safe_mean(before["sorties"]),
            "during_sorties_avg": safe_mean(during["sorties"]),
            "before_n": len(before),
            "during_n": len(during),
        })
    return results


def high_sortie_analysis(merged: pd.DataFrame):
    """
    åˆ†æé«˜æ¶æ¬¡äº‹ä»¶å‰ 7 å¤©çš„æš—èˆ¹æ•¸é‡ã€‚
    é–€æª»ï¼šå¹³å‡ + 2 æ¨™æº–å·®ã€‚
    """
    if len(merged) < 10:
        return {}

    threshold = float(merged["sorties"].mean() + 2 * merged["sorties"].std())
    high_dates = merged.loc[merged["sorties"] > threshold, "date"].tolist()

    if not high_dates:
        return {"threshold": round(threshold, 1), "event_count": 0, "pre_event": {}}

    overall_mean = float(merged["dark_vessels"].mean())
    pre_event = {}
    for days_before in range(1, 8):
        values = []
        for d in high_dates:
            check = d - timedelta(days=days_before)
            row = merged.loc[merged["date"] == check, "dark_vessels"]
            if len(row) > 0:
                values.append(float(row.iloc[0]))
        if values:
            avg = round(np.mean(values), 1)
            diff_pct = round((avg - overall_mean) / overall_mean * 100, 1) if overall_mean > 0 else 0
            pre_event[str(days_before)] = {
                "avg_dark_vessels": avg,
                "diff_from_mean_pct": diff_pct,
            }

    return {
        "threshold": round(threshold, 1),
        "event_count": len(high_dates),
        "overall_dark_mean": round(overall_mean, 1),
        "pre_event": pre_event,
    }


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================


def main():
    print("=" * 60)
    print("ğŸ“Š è»æ¼”é æ¸¬åˆ†æï¼šæš—èˆ¹æ´»å‹• vs PLA æ¶æ¬¡")
    print("=" * 60)

    # è¼‰å…¥è³‡æ–™
    sorties_df = fetch_pla_sorties()
    dark_df = load_dark_vessel_daily()

    if sorties_df.empty or dark_df.empty:
        print("\nâš ï¸ è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œåˆ†æ")
        # å„²å­˜ç©ºçµæœ
        output = {
            "updated_at": datetime.utcnow().isoformat() + "Z",
            "status": "insufficient_data",
        }
        out_path = DATA_DIR / "exercise_prediction.json"
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²å„²å­˜: {out_path}")
        return

    # åˆä½µ
    merged = pd.merge(
        dark_df[["date", "dark_vessels"]],
        sorties_df[["date", "sorties", "vessels"]],
        on="date",
        how="inner",
    )
    print(f"\n   ğŸ“Š åˆä½µè³‡æ–™: {len(merged)} å¤©")

    if len(merged) == 0:
        print("   âš ï¸ æš—èˆ¹èˆ‡æ¶æ¬¡è³‡æ–™æ—¥æœŸç„¡é‡ç–Š")
        output = {
            "updated_at": datetime.utcnow().isoformat() + "Z",
            "status": "no_overlap",
            "dark_range": f"{dark_df['date'].min():%Y-%m-%d} ~ {dark_df['date'].max():%Y-%m-%d}",
            "sorties_range": f"{sorties_df['date'].min():%Y-%m-%d} ~ {sorties_df['date'].max():%Y-%m-%d}",
        }
        out_path = DATA_DIR / "exercise_prediction.json"
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²å„²å­˜: {out_path}")
        return

    # åŸºæœ¬çµ±è¨ˆ
    corr_dark_sorties = float(merged["dark_vessels"].corr(merged["sorties"]))
    corr_dark_vessels = float(merged["dark_vessels"].corr(merged["vessels"]))

    print(f"   ç›¸é—œä¿‚æ•¸: darkâ†”sorties r={corr_dark_sorties:.4f}, darkâ†”PLAN r={corr_dark_vessels:.4f}")

    # æ™‚é–“æ»¯å¾Œç›¸é—œ
    print("\n   ğŸ“Š æ™‚é–“æ»¯å¾Œç›¸é—œåˆ†æ...")
    lag_results = lag_correlation(merged["dark_vessels"], merged["sorties"])
    best = max(lag_results, key=lambda x: abs(x["correlation"]))
    print(f"      æœ€å¤§ç›¸é—œ: lag={best['lag']} å¤©, r={best['correlation']:.4f}")

    # Granger å› æœ
    print("\n   ğŸ“Š Granger å› æœæª¢é©—...")
    granger_results = granger_test(merged.copy())

    sig_lags = [
        k for k, v in granger_results.get("dark_to_sorties", {}).items()
        if v.get("significant")
    ]
    if sig_lags:
        print(f"      æš—èˆ¹â†’æ¶æ¬¡: é¡¯è‘— (lag={sig_lags})")
    else:
        print("      æš—èˆ¹â†’æ¶æ¬¡: ä¸é¡¯è‘—")

    # è»æ¼”äº‹ä»¶ç ”ç©¶
    print("\n   ğŸ“Š è»æ¼”äº‹ä»¶ç ”ç©¶...")
    events = event_study(merged)
    for ev in events:
        if ev["before_n"] > 0 and ev["during_n"] > 0:
            print(f"      {ev['exercise']}: æš—èˆ¹ {ev['before_dark_avg']}â†’{ev['during_dark_avg']} "
                  f"({ev['dark_change_pct']:+.1f}%)" if ev["dark_change_pct"] is not None else
                  f"      {ev['exercise']}: N/A")
        else:
            print(f"      {ev['exercise']}: è³‡æ–™ä¸è¶³")

    # é«˜æ¶æ¬¡å‰æš—èˆ¹åˆ†æ
    print("\n   ğŸ“Š é«˜æ¶æ¬¡å‰æš—èˆ¹åˆ†æ...")
    high_sortie = high_sortie_analysis(merged)
    if high_sortie.get("event_count", 0) > 0:
        print(f"      é–€æª»: >{high_sortie['threshold']} æ¶æ¬¡, {high_sortie['event_count']} äº‹ä»¶")

    # çµ„è£è¼¸å‡º
    output = {
        "updated_at": datetime.utcnow().isoformat() + "Z",
        "status": "ok",
        "data_summary": {
            "merged_days": len(merged),
            "date_range": f"{merged['date'].min():%Y-%m-%d} ~ {merged['date'].max():%Y-%m-%d}",
            "dark_mean": round(float(merged["dark_vessels"].mean()), 1),
            "dark_std": round(float(merged["dark_vessels"].std()), 1),
            "sorties_mean": round(float(merged["sorties"].mean()), 1),
            "sorties_std": round(float(merged["sorties"].std()), 1),
            "correlation_dark_sorties": round(corr_dark_sorties, 4),
            "correlation_dark_plan_vessels": round(corr_dark_vessels, 4),
        },
        "lag_correlation": {
            "max_lag": MAX_LAG,
            "best_lag": best["lag"],
            "best_correlation": best["correlation"],
            "values": lag_results,
        },
        "granger_causality": granger_results,
        "event_study": events,
        "high_sortie_analysis": high_sortie,
        "daily_merged": [
            {
                "date": row["date"].strftime("%Y-%m-%d"),
                "dark_vessels": int(row["dark_vessels"]),
                "sorties": float(row["sorties"]),
                "vessels": float(row["vessels"]),
            }
            for _, row in merged.iterrows()
        ],
    }

    out_path = DATA_DIR / "exercise_prediction.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… åˆ†æçµæœå·²å„²å­˜: {out_path}")


if __name__ == "__main__":
    main()
