#!/usr/bin/env python3
"""
================================================================================
æš—èˆ¹å‹•ç•«è³‡æ–™æ“·å–è…³æœ¬
Dark Vessel Animation Data Fetcher
================================================================================

åŠŸèƒ½ï¼š
1. å¾ GFW API æ“·å– 90 å¤© SAR è¡›æ˜Ÿåµæ¸¬è³‡æ–™ï¼ˆ4 å€‹å­å€åŸŸï¼‰
2. æŒ‰å¯¦éš›åµæ¸¬æ—¥åˆ†çµ„ï¼ˆè·³éç„¡è³‡æ–™çš„æ—¥æœŸï¼‰
3. è¼¸å‡º JSON ä¾›å‰ç«¯å‹•ç•«é é¢ä½¿ç”¨

è¼¸å‡ºï¼šdata/weekly_dark_vessels.json
================================================================================
"""

import os
import sys
import json
import time
import requests
from datetime import datetime, timedelta
from pathlib import Path

# =============================================================================
# è¨­å®š
# =============================================================================

API_TOKEN = os.environ.get('GFW_API_TOKEN', '').strip()
BASE_URL = "https://gateway.api.globalfishingwatch.org/v3"

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

OUTPUT_PATH = DATA_DIR / 'weekly_dark_vessels.json'

# è³‡æ–™å¤©æ•¸ç¯„åœï¼ˆé è¨­ 90 å¤©ï¼‰
DATA_RANGE_DAYS = 90

# =============================================================================
# å€åŸŸå®šç¾©ï¼ˆèˆ‡ fetch_gfw_data.py åŒæ­¥ï¼Œæ±æµ·å·²æ“´å±•è‡³ 34Â°N/130.5Â°Eï¼‰
# =============================================================================

DARK_VESSEL_REGIONS = {
    "taiwan_strait": {
        "name": "å°ç£æµ·å³½",
        "geojson": {
            "type": "Polygon",
            "coordinates": [[
                [118.0, 23.5], [122.0, 23.5], [122.0, 26.5], [118.0, 26.5], [118.0, 23.5]
            ]]
        }
    },
    "east_taiwan": {
        "name": "å°ç£æ±éƒ¨æµ·åŸŸ",
        "geojson": {
            "type": "Polygon",
            "coordinates": [[
                [121.5, 22.0], [124.0, 22.0], [124.0, 25.5], [121.5, 25.5], [121.5, 22.0]
            ]]
        }
    },
    "south_china_sea": {
        "name": "å—æµ·åŒ—éƒ¨",
        "geojson": {
            "type": "Polygon",
            "coordinates": [[
                [110.0, 18.0], [118.0, 18.0], [118.0, 23.0], [110.0, 23.0], [110.0, 18.0]
            ]]
        }
    },
    "east_china_sea": {
        "name": "æ±æµ·",
        "geojson": {
            "type": "Polygon",
            "coordinates": [[
                [122.0, 26.0], [130.5, 26.0], [130.5, 34.0], [122.0, 34.0], [122.0, 26.0]
            ]]
        }
    }
}

# =============================================================================
# API å‡½æ•¸
# =============================================================================

def get_headers():
    return {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }


def fetch_sar_data(region_geojson, start_date, end_date):
    """æ“·å– SAR è¡›æ˜Ÿåµæ¸¬è³‡æ–™"""
    params = {
        "datasets[0]": "public-global-sar-presence:latest",
        "date-range": f"{start_date},{end_date}",
        "temporal-resolution": "DAILY",
        "spatial-resolution": "HIGH",
        "spatial-aggregation": "false",
        "format": "JSON"
    }

    try:
        response = requests.post(
            f"{BASE_URL}/4wings/report",
            params=params,
            json={"geojson": region_geojson},
            headers=get_headers(),
            timeout=180
        )

        if response.status_code == 200:
            data = response.json()
            entries = data.get('entries', [])
            records = []
            for entry in entries:
                for key, values in entry.items():
                    if isinstance(values, list):
                        records.extend(values)
            return records
        else:
            print(f"   âŒ API éŒ¯èª¤ {response.status_code}: {response.text[:200]}")
            return []

    except Exception as e:
        print(f"   âŒ è«‹æ±‚å¤±æ•—: {e}")
        return []


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================

def main():
    print("=" * 70)
    print("ğŸ¬ æš—èˆ¹å‹•ç•«è³‡æ–™æ“·å– - Dark Vessel Animation Data")
    print("=" * 70)
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")

    if not API_TOKEN:
        print("âš ï¸ æœªè¨­å®š GFW_API_TOKENï¼Œè·³é")
        return

    # 12 å°æ™‚æ–°é®®åº¦æª¢æŸ¥
    if OUTPUT_PATH.exists():
        mod_time = datetime.fromtimestamp(OUTPUT_PATH.stat().st_mtime)
        age_hours = (datetime.now() - mod_time).total_seconds() / 3600
        if age_hours < 12:
            print(f"â­ï¸ {OUTPUT_PATH} å°šæ–°ï¼ˆ{age_hours:.1f}hï¼‰ï¼Œè·³éé‡æ–°æ“·å–")
            return

    # è¨ˆç®—æ—¥æœŸç¯„åœ
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=DATA_RANGE_DAYS)
    start_str = start_date.strftime('%Y-%m-%d')
    end_str = end_date.strftime('%Y-%m-%d')

    print(f"ğŸ“… æŸ¥è©¢ç¯„åœ: {start_str} ~ {end_str} ({DATA_RANGE_DAYS} å¤©)")

    # æ”¶é›†æ‰€æœ‰å€åŸŸçš„æš—èˆ¹è¨˜éŒ„
    all_records = []  # (region_id, region_name, record)

    for region_id, region_info in DARK_VESSEL_REGIONS.items():
        print(f"\nğŸ“ {region_info['name']}...")
        records = fetch_sar_data(region_info['geojson'], start_str, end_str)
        print(f"   å–å¾— {len(records)} ç­† SAR è¨˜éŒ„")

        for r in records:
            all_records.append((region_id, region_info['name'], r))

        time.sleep(2)  # API é€Ÿç‡é™åˆ¶

    print(f"\nğŸ“Š ç¸½å…±å–å¾— {len(all_records)} ç­†è¨˜éŒ„")

    # æŒ‰æ—¥æœŸåˆ†çµ„
    days_map = {}  # date -> {regions -> {dark_count, total_count, points}}

    for region_id, region_name, record in all_records:
        date = record.get('date', '')[:10]
        if not date:
            continue

        is_dark = not record.get('vesselId')

        if date not in days_map:
            days_map[date] = {}

        if region_id not in days_map[date]:
            days_map[date][region_id] = {
                'name': region_name,
                'dark_count': 0,
                'total_count': 0,
                'points': []
            }

        region_data = days_map[date][region_id]
        region_data['total_count'] += 1

        if is_dark:
            region_data['dark_count'] += 1
            lat = record.get('lat', record.get('latitude'))
            lon = record.get('lon', record.get('longitude'))
            if lat is not None and lon is not None:
                region_data['points'].append({
                    'lat': round(lat, 2),
                    'lon': round(lon, 2),
                    'detections': record.get('detections', 1)
                })

    # çµ„è£è¼¸å‡º JSON
    days_list = []
    for date in sorted(days_map.keys()):
        regions = days_map[date]
        total_detections = sum(r['total_count'] for r in regions.values())
        dark_vessels = sum(r['dark_count'] for r in regions.values())
        point_count = sum(len(r['points']) for r in regions.values())

        days_list.append({
            'date': date,
            'summary': {
                'total_detections': total_detections,
                'dark_vessels': dark_vessels,
                'dark_ratio': round(dark_vessels / total_detections * 100, 1) if total_detections > 0 else 0,
                'point_count': point_count
            },
            'regions': regions
        })

    output = {
        'updated_at': datetime.utcnow().isoformat() + 'Z',
        'date_range': {
            'start': start_str,
            'end': end_str
        },
        'total_days': len(days_list),
        'days': days_list
    }

    # å„²å­˜
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å‹•ç•«è³‡æ–™å·²å„²å­˜: {OUTPUT_PATH}")
    print(f"   åµæ¸¬å¤©æ•¸: {len(days_list)}")
    total_points = sum(d['summary']['point_count'] for d in days_list)
    print(f"   æš—èˆ¹åº§æ¨™é»: {total_points}")
    print(f"   æª”æ¡ˆå¤§å°: {OUTPUT_PATH.stat().st_size / 1024:.1f} KB")


if __name__ == "__main__":
    main()
